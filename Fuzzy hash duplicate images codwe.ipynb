{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2e6ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ImageHash in c:\\users\\pruthviraj.m\\appdata\\roaming\\python\\python39\\site-packages (4.3.0)\n",
      "Requirement already satisfied: scipy in d:\\programdata\\anaconda3\\lib\\site-packages (from ImageHash) (1.7.3)\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\lib\\site-packages (from ImageHash) (1.21.5)\n",
      "Requirement already satisfied: PyWavelets in d:\\programdata\\anaconda3\\lib\\site-packages (from ImageHash) (1.3.0)\n",
      "Requirement already satisfied: pillow in d:\\programdata\\anaconda3\\lib\\site-packages (from ImageHash) (9.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ImageHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbbb3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    return image_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d8c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def resize_images(folder_path, target_size=(128, 128)):\n",
    "    image_files = get_image_files(folder_path)\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        img = img.resize(target_size)\n",
    "        img.save(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7445ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "\n",
    "def compute_image_hashes(folder_path):\n",
    "    image_files = get_image_files(folder_path)\n",
    "    image_hashes = {}\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        hash_code = str(imagehash.phash(img))\n",
    "        if hash_code in image_hashes:\n",
    "            image_hashes[hash_code].append(image_path)\n",
    "        else:\n",
    "            image_hashes[hash_code] = [image_path]\n",
    "    return image_hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0bef8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate images found.\n"
     ]
    }
   ],
   "source": [
    "def find_duplicate_images(folder_path):\n",
    "    image_hashes = compute_image_hashes(folder_path)\n",
    "    duplicate_images = [paths for paths in image_hashes.values() if len(paths) > 1]\n",
    "    return duplicate_images\n",
    "\n",
    "folder_path = r\"\\\\mdibkstorage\\DataAnalytics&Infographics\\TEST FILES\"\n",
    "resize_images(folder_path)\n",
    "duplicate_images = find_duplicate_images(folder_path)\n",
    "\n",
    "if duplicate_images:\n",
    "    print(\"Duplicate images found:\")\n",
    "    for duplicate_set in duplicate_images:\n",
    "        print(\"Duplicate set:\")\n",
    "        for image_path in duplicate_set:\n",
    "            print(image_path)\n",
    "else:\n",
    "    print(\"No duplicate images found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115395f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate images found:\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"\\\\mdibkstorage\\DataAnalytics&Infographics\\TEST FILES\n",
    "resize_images(folder_path)\n",
    "duplicate_images = find_duplicate_images(folder_path)\n",
    "print(\"Duplicate images found:\")\n",
    "for images in duplicate_images:\n",
    "    print(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85977440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate images found.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from imagehash import average_hash, phash, dhash, whash\n",
    "\n",
    "def get_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    return image_files\n",
    "\n",
    "def compute_image_hashes(folder_path):\n",
    "    image_files = get_image_files(folder_path)\n",
    "    image_hashes = {}\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = Image.open(image_path)\n",
    "        hash_code = str(average_hash(img))  # You can use phash, dhash, or whash instead of average_hash\n",
    "        if hash_code in image_hashes:\n",
    "            image_hashes[hash_code].append(image_path)\n",
    "        else:\n",
    "            image_hashes[hash_code] = [image_path]\n",
    "    return image_hashes\n",
    "\n",
    "def find_duplicate_images(folder_path):\n",
    "    image_hashes = compute_image_hashes(folder_path)\n",
    "    duplicate_images = [paths for paths in image_hashes.values() if len(paths) > 1]\n",
    "    return duplicate_images\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"\\\\mdibkstorage\\DataAnalytics&Infographics\\TEST FILES\"\n",
    "    duplicate_images = find_duplicate_images(folder_path)\n",
    "\n",
    "    if duplicate_images:\n",
    "        print(\"Duplicate images found:\")\n",
    "        for duplicate_set in duplicate_images:\n",
    "            print(\"Duplicate set:\")\n",
    "            for image_path in duplicate_set:\n",
    "                print(image_path)\n",
    "    else:\n",
    "        print(\"No duplicate images found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed94951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: scikit-image in d:\\programdata\\anaconda3\\lib\\site-packages (0.19.2)\n",
      "Collecting scikit-fuzzy\n",
      "  Downloading scikit-fuzzy-0.4.2.tar.gz (993 kB)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2.9.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (9.0.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (1.7.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: networkx>=2.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image) (2.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->scikit-image) (3.0.4)\n",
      "Building wheels for collected packages: scikit-fuzzy\n",
      "  Building wheel for scikit-fuzzy (setup.py): started\n",
      "  Building wheel for scikit-fuzzy (setup.py): finished with status 'done'\n",
      "  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-py3-none-any.whl size=894089 sha256=aaf933a575f3dc77240ec72c15a52cff487bece778b9e2fdb9d9ed9812388c3c\n",
      "  Stored in directory: c:\\users\\pruthviraj.m\\appdata\\local\\pip\\cache\\wheels\\32\\2c\\a1\\a90a7d7dd8448ec029f298a61f3490275e99b17aa348be675c\n",
      "Successfully built scikit-fuzzy\n",
      "Installing collected packages: scikit-fuzzy\n",
      "Successfully installed scikit-fuzzy-0.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy scikit-image scikit-fuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4cfcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate images:\n",
      "['adfag.pdf_page_1.png', 'daf (1).jpg', 'daf (2).jpg', 'dafdsafgewbbb (1).jpg', 'dafdsafgewbbb (12).jpg', 'dafdsafgewbbb (2).jpg', 'dafdsafgewbbb (7).jpg', 'dafdsafgewbbb (8).jpg', 'dafdsafgewbbb (9).jpg', 'dagfrvbs (1).jpg', 'dagfrvbs (2).jpg', 'dagfrvbs (3).jpg', 'dagfrvbs (4).jpg']\n",
      "Similar images:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage import io, color, img_as_ubyte\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "def get_image_files(folder_path):\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif']\n",
    "    image_files = [f for f in os.listdir(folder_path) if os.path.splitext(f)[1].lower() in image_extensions]\n",
    "    return image_files\n",
    "\n",
    "def compute_image_features(folder_path):\n",
    "    image_files = get_image_files(folder_path)\n",
    "    image_features = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        img = io.imread(image_path)\n",
    "        gray_img = color.rgb2gray(img)\n",
    "        # Extract a simple feature (mean intensity value) for demonstration purposes\n",
    "        feature = np.mean(gray_img)\n",
    "        image_features.append(feature)\n",
    "    return np.array(image_features)\n",
    "\n",
    "def fuzzy_similarity(features):\n",
    "    # Define the fuzzy sets\n",
    "    similarity = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "    # Define the membership functions for each set\n",
    "    similarity_very_similar = fuzz.trapmf(similarity, [0, 0, 0.2, 0.5])\n",
    "    similarity_somewhat_similar = fuzz.trimf(similarity, [0.2, 0.5, 0.8])\n",
    "    similarity_less_similar = fuzz.trapmf(similarity, [0.5, 0.8, 1, 1])\n",
    "\n",
    "    # Calculate membership degrees for each image feature\n",
    "    degrees_very_similar = fuzz.interp_membership(similarity, similarity_very_similar, features)\n",
    "    degrees_somewhat_similar = fuzz.interp_membership(similarity, similarity_somewhat_similar, features)\n",
    "    degrees_less_similar = fuzz.interp_membership(similarity, similarity_less_similar, features)\n",
    "\n",
    "    # Calculate the overall similarity scores\n",
    "    similarity_score_very_similar = np.max(degrees_very_similar)\n",
    "    similarity_score_somewhat_similar = np.max(degrees_somewhat_similar)\n",
    "    similarity_score_less_similar = np.max(degrees_less_similar)\n",
    "\n",
    "    return similarity_score_very_similar, similarity_score_somewhat_similar, similarity_score_less_similar\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"\\\\mdibkstorage\\DataAnalytics&Infographics\\TEST FILES\"\n",
    "    image_features = compute_image_features(folder_path)\n",
    "    similarity_score_very_similar, similarity_score_somewhat_similar, similarity_score_less_similar = fuzzy_similarity(image_features)\n",
    "\n",
    "    # Thresholds for similarity levels\n",
    "    threshold_very_similar = 0.5\n",
    "    threshold_somewhat_similar = 0.5\n",
    "\n",
    "    duplicate_images = [f for f, sim in zip(get_image_files(folder_path), image_features) if sim >= threshold_very_similar]\n",
    "    similar_images = [f for f, sim in zip(get_image_files(folder_path), image_features) if threshold_somewhat_similar <= sim < threshold_very_similar]\n",
    "\n",
    "    print(\"Duplicate images:\")\n",
    "    print(duplicate_images)\n",
    "    print(\"Similar images:\")\n",
    "    print(similar_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8332bf54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
